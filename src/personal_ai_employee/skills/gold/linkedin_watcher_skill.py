#!/usr/bin/env python3
"""
Personal AI Employee - LinkedIn Watcher Skill
Gold Tier - G-M3: Social Watchers

Purpose: LinkedIn notification/message perception-only watcher
Tier: Gold
Skill ID: G-M3-T04

CRITICAL: PERCEPTION-ONLY. NEVER posts, NEVER replies, NEVER sends messages.
Only creates intake wrappers in Social/Inbox/.
"""

import os
import sys
import json
import argparse
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Optional

# Import PII redaction and helpers
try:
    from personal_ai_employee.core.mcp_helpers import redact_pii, get_repo_root
except ImportError:
    import re
    def redact_pii(text: str) -> str:
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        phone_pattern = r'(\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}'
        text = re.sub(email_pattern, '[EMAIL_REDACTED]', text)
        text = re.sub(phone_pattern, '[PHONE_REDACTED]', text)
        return text

    def get_repo_root() -> Path:
        """Fallback: find repo root"""
        current = Path(__file__).parent
        while current != current.parent:
            if (current / 'system_log.md').exists():
                return current
            current = current.parent
        return Path(__file__).parent.parent.parent.parent.parent

# Import LinkedIn API helper for real mode
try:
    from personal_ai_employee.core.linkedin_api_helper import LinkedInAPIHelper
except ImportError:
    # If running as script without proper sys.path
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    try:
        from personal_ai_employee.core.linkedin_api_helper import LinkedInAPIHelper
    except ImportError:
        LinkedInAPIHelper = None


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LinkedInWatcher:
    """LinkedIn Watcher - Perception-only intake"""

    def __init__(self, config: Dict):
        self.config = config
        self.checkpoint_data = self._load_checkpoint()
        self.created_count = 0
        self.skipped_count = 0
        self.errors = []

    def _load_checkpoint(self) -> Dict:
        checkpoint_path = self.config['checkpoint_path']
        if os.path.exists(checkpoint_path):
            try:
                with open(checkpoint_path, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load checkpoint: {e}")
                return {'processed_ids': [], 'last_run': None}
        return {'processed_ids': [], 'last_run': None}

    def _save_checkpoint(self):
        checkpoint_path = self.config['checkpoint_path']
        self.checkpoint_data['last_run'] = datetime.now(timezone.utc).isoformat()

        if len(self.checkpoint_data['processed_ids']) > 500:
            self.checkpoint_data['processed_ids'] = self.checkpoint_data['processed_ids'][-500:]

        try:
            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
            with open(checkpoint_path, 'w') as f:
                json.dump(self.checkpoint_data, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to save checkpoint: {e}")

    def reset_checkpoint(self):
        self.checkpoint_data = {'processed_ids': [], 'last_run': None}
        self._save_checkpoint()
        print("✓ LinkedIn checkpoint reset")

    def _create_remediation_task(self, title: str, description: str):
        """Create remediation task in Needs_Action/ when MCP fails"""
        try:
            timestamp = datetime.now(timezone.utc).strftime('%Y%m%d-%H%M')
            filename = f"remediation__mcp__linkedin__{timestamp}.md"
            task_path = Path(self.config['base_dir']) / 'Needs_Action' / filename

            task_content = f"""---
id: remediation_linkedin_{timestamp}
type: remediation
source: linkedin_watcher
created_at: {datetime.now(timezone.utc).isoformat()}
priority: high
status: needs_action
---

# MCP Remediation: {title}

**Issue**: {title}
**Source**: LinkedIn Watcher (G-M3)
**Created**: {datetime.now(timezone.utc).isoformat()}

## Problem

{description}

## Suggested Actions

1. Check `.secrets/linkedin_credentials.json` exists and has valid credentials
2. Verify LinkedIn MCP server is running and reachable
3. Review Docs/mcp_linkedin_setup.md for setup instructions
4. Test connection: `python3 brain_mcp_registry_refresh_skill.py --server linkedin --mock`
5. Re-run watcher in mock mode to verify intake wrapper creation works

## References

- Setup Guide: Docs/mcp_linkedin_setup.md
- Credentials Template: .secrets/README.md
- MCP Registry: Logs/mcp_tools_snapshot_linkedin.json

---
**Generated by**: linkedin_watcher_skill.py
**Graceful Degradation**: Watcher continues running despite MCP failure
"""

            task_path.parent.mkdir(parents=True, exist_ok=True)
            task_path.write_text(task_content, encoding='utf-8')

            logger.info(f"Created remediation task: {filename}")
            self._append_log(f"Remediation task created: {title}")

        except Exception as e:
            logger.error(f"Failed to create remediation task: {e}")

    def _append_log(self, message: str):
        log_path = self.config['log_path']
        try:
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
            timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
            with open(log_path, 'a', encoding='utf-8') as f:
                f.write(f"[{timestamp}] {message}\n")
        except Exception as e:
            logger.warning(f"Could not write to log: {e}")

    def _create_intake_wrapper(self, item: Dict, dry_run: bool = False) -> Optional[str]:
        try:
            item_id = item['id']
            sender = item.get('sender', 'unknown')
            timestamp = datetime.now(timezone.utc)
            timestamp_str = timestamp.strftime('%Y%m%d-%H%M')

            # Extract username from LinkedIn URL
            sender_safe = sender.split('/')[-1][:20]

            filename = f"inbox__linkedin__{timestamp_str}__{sender_safe}.md"
            output_path = os.path.join(self.config['output_dir'], filename)

            if dry_run:
                print(f"[DRY-RUN] Would create: {filename}")
                return None

            body = item.get('body', '')
            sender_name = item.get('sender_name', sender_safe)
            item_type = item.get('type', 'notification')
            post_id = item.get('post_id', '')
            comment_id = item.get('comment_id', '')
            thread_id = item.get('thread_id', f"linkedin_{sender_safe}")
            is_urgent = item.get('urgent', False)

            # Privacy-safe excerpt
            excerpt = body[:500] if len(body) <= 500 else body[:497] + "..."
            excerpt = redact_pii(excerpt)

            frontmatter = f"""---
id: {item_id}
source: linkedin
received_at: {item.get('timestamp', timestamp.isoformat())}
sender: {sender}
channel: linkedin
thread_id: {thread_id}
excerpt: "{excerpt}"
status: pending
plan_required: {str(is_urgent).lower()}
pii_redacted: true
linkedin_type: {item_type}
post_id: {post_id}
comment_id: {comment_id}
---

# LinkedIn {item_type.title()} - {sender_name}

**⚠️ PERCEPTION-ONLY**: This is an intake wrapper. No LinkedIn actions have been taken.

## Item Details

**From**: {sender_name}
**Profile**: {sender}
**Received**: {item.get('timestamp', timestamp.isoformat())}
**Type**: {item_type}
**Post ID**: {post_id or 'N/A'}
**Comment ID**: {comment_id or 'N/A'}
**Thread**: {thread_id}
**Urgent**: {is_urgent}

## Content

{body}

## Suggested Next Steps

**If actionable (reply/comment/message)**:
1. Create plan using `brain_create_plan` (Gold Tier - G-M4)
2. Request approval via `brain_request_approval`
3. Execute via `brain_execute_social_with_mcp` (LinkedIn action)

**If informational only**:
- Archive to Done/ with summary note

## Privacy Notice

- PII in excerpt redacted for privacy
- Full content preserved above for context
- Never commit to git (covered by .gitignore)

---
**Generated by**: linkedin_watcher
**Watcher Version**: 1.0.0 (Gold Tier - G-M3)
"""

            os.makedirs(self.config['output_dir'], exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(frontmatter)

            logger.info(f"Created intake wrapper: {filename}")
            return output_path

        except Exception as e:
            logger.error(f"Failed to create intake wrapper: {e}")
            self.errors.append(str(e))
            return None

    def _load_mock_items(self) -> List[Dict]:
        fixture_path = self.config['mock_fixture_path']

        if not os.path.exists(fixture_path):
            raise FileNotFoundError(f"Mock fixture not found: {fixture_path}")

        try:
            with open(fixture_path, 'r') as f:
                items = json.load(f)

            if not isinstance(items, list):
                raise ValueError("Mock fixture must be a JSON array")

            logger.info(f"Loaded {len(items)} mock LinkedIn items")
            return items

        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in mock fixture: {e}")

    def _fetch_items_real(self) -> List[Dict]:
        """
        Fetch posts from real LinkedIn API using OAuth2 helper (QUERY only).

        IMPORTANT: This ONLY calls QUERY methods (list_ugc_posts).
        NEVER calls ACTION methods (create_post, reply_comment, send_message).

        Returns:
            List of post dicts in standard format for intake wrapper creation
        """
        try:
            if LinkedInAPIHelper is None:
                logger.error("LinkedInAPIHelper not available (import failed)")
                self._create_remediation_task(
                    "LinkedIn API helper import failed",
                    "Could not import LinkedInAPIHelper. Check src/personal_ai_employee/core/linkedin_api_helper.py exists"
                )
                return []

            # Initialize helper (will check for credentials and token)
            secrets_dir = Path(self.config['base_dir']) / '.secrets'
            helper = LinkedInAPIHelper(secrets_dir=secrets_dir)

            # Verify authentication before proceeding
            try:
                auth_info = helper.check_auth()
                logger.info(f"LinkedIn auth OK: {auth_info.get('localizedFirstName', 'User')}")
            except Exception as auth_error:
                logger.warning(f"LinkedIn authentication failed: {auth_error}")
                self._create_remediation_task(
                    "LinkedIn authentication required",
                    f"Real mode requires valid LinkedIn OAuth2 token.\n\n"
                    f"Error: {auth_error}\n\n"
                    f"To authenticate:\n"
                    f"1. Run: python3 src/personal_ai_employee/core/linkedin_api_helper.py\n"
                    f"2. Follow OAuth2 flow and paste callback URL\n"
                    f"3. Token will be saved to .secrets/linkedin_token.json\n"
                    f"4. Re-run watcher in real mode\n\n"
                    f"See Docs/linkedin_real_setup.md for details."
                )
                return []

            # Fetch UGC posts (read-only perception)
            # Note: LinkedIn API requires author URN which we get from auth
            author_urn = auth_info.get('id', '')
            if not author_urn:
                logger.warning("Could not determine LinkedIn author URN from auth response")
                return []

            logger.info(f"Fetching LinkedIn UGC posts for author: {author_urn}")
            max_results = self.config.get('max_results', 10)
            posts = helper.list_ugc_posts(author_urn=f"urn:li:person:{author_urn}", limit=max_results)

            # Transform LinkedIn API response to standard format
            items = []
            for post in posts:
                # Extract relevant fields from LinkedIn UGC post response
                post_id = post.get('id', '')
                created_time = post.get('created', {}).get('time', datetime.now(timezone.utc).timestamp() * 1000)
                created_iso = datetime.fromtimestamp(created_time / 1000, tz=timezone.utc).isoformat()

                # Extract text from specificContent
                text_content = ""
                specific_content = post.get('specificContent', {})
                share_content = specific_content.get('com.linkedin.ugc.ShareContent', {})
                share_commentary = share_content.get('shareCommentary', {})
                text_content = share_commentary.get('text', '')

                # Get author info
                author = post.get('author', '')

                item = {
                    'id': post_id,
                    'sender': f"https://www.linkedin.com/feed/update/{post_id}",
                    'sender_name': 'You',  # Since we're fetching our own posts
                    'timestamp': created_iso,
                    'body': text_content,
                    'type': 'post',
                    'post_id': post_id,
                    'comment_id': '',
                    'thread_id': f"linkedin_{post_id}",
                    'urgent': False
                }
                items.append(item)

            logger.info(f"Fetched {len(items)} LinkedIn posts from API")
            return items

        except Exception as e:
            logger.error(f"Failed to fetch from LinkedIn API: {e}")
            self._create_remediation_task(
                "LinkedIn API query failed",
                f"Error: {e}\n\nCheck:\n"
                f"1. .secrets/linkedin_credentials.json exists with valid client_id/client_secret\n"
                f"2. .secrets/linkedin_token.json exists with valid access_token\n"
                f"3. Token hasn't expired (run linkedin_api_helper.py to refresh)\n"
                f"4. Network connection is working\n\n"
                f"See Docs/linkedin_real_setup.md for troubleshooting."
            )
            return []

    def run(self, dry_run: bool = False, mock: bool = True, verbose: bool = False) -> Dict:
        start_time = datetime.now(timezone.utc)

        if verbose:
            logger.setLevel(logging.DEBUG)

        logger.info(f"Starting LinkedIn watcher (mock={mock}, dry_run={dry_run})")

        if mock:
            try:
                items = self._load_mock_items()
            except Exception as e:
                logger.error(f"Failed to load mock items: {e}")
                return {'success': False, 'error': str(e)}
        else:
            items = self._fetch_items_real()

        max_results = self.config.get('max_results', 10)
        items = items[:max_results]

        logger.info(f"Processing {len(items)} LinkedIn items")

        for item in items:
            item_id = item.get('id')

            if not item_id:
                logger.warning("Item missing ID, skipping")
                continue

            if item_id in self.checkpoint_data['processed_ids']:
                logger.debug(f"Skipping already processed: {item_id}")
                self.skipped_count += 1
                continue

            output_path = self._create_intake_wrapper(item, dry_run=dry_run)

            if output_path:
                self.created_count += 1
                if not dry_run:
                    self.checkpoint_data['processed_ids'].append(item_id)

        if not dry_run and self.created_count > 0:
            self._save_checkpoint()

        duration = (datetime.now(timezone.utc) - start_time).total_seconds()
        summary = (
            f"LinkedIn watcher complete: "
            f"{len(items)} scanned, {self.created_count} created, "
            f"{self.skipped_count} skipped, {len(self.errors)} errors, "
            f"{duration:.2f}s"
        )

        self._append_log(summary)

        try:
            with open('system_log.md', 'a', encoding='utf-8') as f:
                f.write(f"\n**[{start_time.isoformat()}]** {summary}\n")
        except:
            pass

        print(f"\n✅ {summary}")

        return {
            'success': True,
            'scanned': len(items),
            'created': self.created_count,
            'skipped': self.skipped_count,
            'errors': len(self.errors),
            'duration': duration
        }


def main():
    parser = argparse.ArgumentParser(description='LinkedIn Watcher - Perception-only (Gold Tier - Real Mode)')
    parser.add_argument('--once', action='store_true', default=True, help='Run once and exit (default)')
    parser.add_argument('--dry-run', action='store_true', help='Simulate run without creating files')
    parser.add_argument('--mode', type=str, choices=['mock', 'real'], default='mock',
                        help='Data source mode: mock (default, uses templates/mock_linkedin.json) or real (uses LinkedIn OAuth2 API)')
    parser.add_argument('--max-results', type=int, default=10, help='Maximum items to process (default: 10)')
    parser.add_argument('--reset-checkpoint', action='store_true', help='Reset checkpoint before running')
    parser.add_argument('--verbose', action='store_true', help='Enable verbose logging')

    args = parser.parse_args()

    # Determine mock vs real mode
    use_mock = (args.mode == 'mock')

    config = {
        'base_dir': get_repo_root(),
        'checkpoint_path': 'Logs/linkedin_watcher_checkpoint.json',
        'log_path': 'Logs/linkedin_watcher.log',
        'output_dir': 'Social/Inbox',
        'max_results': args.max_results,
        'mock_fixture_path': 'templates/mock_linkedin.json'
    }

    watcher = LinkedInWatcher(config)

    if args.reset_checkpoint:
        watcher.reset_checkpoint()

    result = watcher.run(dry_run=args.dry_run, mock=use_mock, verbose=args.verbose)

    return 0 if result['success'] else 1


if __name__ == "__main__":
    sys.exit(main())
